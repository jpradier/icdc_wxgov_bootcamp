{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d4be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO UPDATE THE ID BEFORE RUNING THE NOTEBOOK\n",
    "## If you are unsure how to collect the different ID, please ask your instructor\n",
    "\n",
    "# Your IBM Cloud API Key, required to authenticate yourself when accessing the services using the python SDK\n",
    "CLOUD_API_KEY = \"*************************\"\n",
    "\n",
    "# Your ModelManagement/OpenScale Instance ID that has been provisionned for you\n",
    "OPENSCALE_INSTANCE_ID = \"*************************\" # Update this to refer to a particular service instance\n",
    "\n",
    "# The watsonx.ai production deployement space id where the Agentic RAG Langchain  model is deployed\n",
    "SPACE_ID = \"*************************\"\n",
    "\n",
    "# The unique name of the watsonx.ai Agentic RAG Langchain Developement like \"Agentic RAG using LangChain\"\n",
    "AGENTIC_RAG_LC_DEV_NAME = \"*************************\"\n",
    "\n",
    "# The prompt template asset id created in the watsonx.ai space for the Use Case\n",
    "SPACE_PROMPT_TEMPLATE_ID = \"*************************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792561f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ibm-watson-openscale | tail -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b022cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IAM_URL = \"https://iam.cloud.ibm.com\"\n",
    "FACTSHEET_URL = \"https://eu-de.dataplatform.cloud.ibm.com\"\n",
    "SERVICE_URL = \"https://eu-de.aiopenscale.cloud.ibm.com\"\n",
    "\n",
    "credentials = {\n",
    "    \"url\": \"https://eu-de.ml.cloud.ibm.com\",\n",
    "    \"apikey\": CLOUD_API_KEY,\n",
    "}\n",
    "CREDENTIALS = credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1841430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator, CloudPakForDataAuthenticator\n",
    "\n",
    "from ibm_watson_openscale import *\n",
    "from ibm_watson_openscale.supporting_classes.enums import *\n",
    "from ibm_watson_openscale.supporting_classes import *\n",
    "\n",
    "SERVICE_URL=\"https://eu-de.aiopenscale.cloud.ibm.com\"\n",
    "\n",
    "authenticator = IAMAuthenticator(\n",
    "    apikey=CLOUD_API_KEY,\n",
    "    url=IAM_URL\n",
    ")\n",
    "wos_client = APIClient(\n",
    "    authenticator=authenticator,\n",
    "    service_url=SERVICE_URL,\n",
    "    service_instance_id=OPENSCALE_INSTANCE_ID\n",
    ")\n",
    "data_mart_id = wos_client.service_instance_id\n",
    "print(wos_client.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fead7933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "gen_ai_evaluator = wos_client.integrated_systems.add(\n",
    "    name=\"RAG Metrics Evaluator\",\n",
    "    description=\"RAG Metrics Evaluator\",\n",
    "    type=\"generative_ai_evaluator\",\n",
    "    parameters={\"evaluator_type\": \"watsonx.ai\", \"model_id\": \"meta-llama/llama-3-3-70b-instruct\"},\n",
    "    credentials={\n",
    "        \"url\": \"https://eu-de.ml.cloud.ibm.com\",\n",
    "        \"wml_location\": \"cloud\",\n",
    "        \"apikey\": CLOUD_API_KEY,\n",
    "    },\n",
    ")\n",
    "\n",
    "# get evaluator integrated system ID\n",
    "result = gen_ai_evaluator.result._to_dict()\n",
    "evaluator_id = result[\"metadata\"][\"id\"]\n",
    "evaluator_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f53be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_column = \"reference\"\n",
    "context_fields = [\"context\"]\n",
    "question_field = \"query\"\n",
    "operational_space_id = \"production\"\n",
    "problem_type= \"retrieval_augmented_generation\"\n",
    "input_data_type= \"unstructured_text\"\n",
    "\n",
    "monitors = {\n",
    "    \"generative_ai_quality\": {\n",
    "        \"parameters\": {\n",
    "            \"generative_ai_evaluator\": { # global LLM as judge configuration\n",
    "               \"enabled\": True,\n",
    "               \"evaluator_id\": evaluator_id,\n",
    "            },\n",
    "            \"min_sample_size\": 2,\n",
    "            \"metrics_configuration\": {\n",
    "                \"faithfulness\": {\n",
    "                    # \"metric_prompt_template\": \"\", # adding custom template\n",
    "                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric.\n",
    "                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n",
    "                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n",
    "                    #     \"enabled\": True,\n",
    "                    #     \"evaluator_id\": evaluator_id,\n",
    "                    # },\n",
    "                },\n",
    "                \"answer_relevance\": {\n",
    "                    # \"metric_prompt_template\": \"\", # adding custom template\n",
    "                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric\n",
    "                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n",
    "                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n",
    "                    #     \"enabled\": True,\n",
    "                    #     \"evaluator_id\": evaluator_id,\n",
    "                    # },\n",
    "                },\n",
    "                \"rouge_score\": {},\n",
    "                \"exact_match\": {},\n",
    "                \"bleu\": {},\n",
    "                \"unsuccessful_requests\": {\n",
    "                    # \"unsuccessful_phrases\": []\n",
    "                },\n",
    "                \"hap_input_score\": {},\n",
    "                \"hap_score\": {},\n",
    "                \"pii\": {},\n",
    "                \"pii_input\": {},\n",
    "                \"retrieval_quality\": {\n",
    "                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric\n",
    "                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n",
    "                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n",
    "                    #     \"enabled\": True,\n",
    "                    #     \"evaluator_id\": evaluator_id,\n",
    "                    # },\n",
    "                    # The metrics computed for retrieval quality are context_relevance, retrieval_precision, average_precision, reciprocal_rank, hit_rate, normalized_discounted_cumulative_gain\n",
    "                    # \"context_relevance\": {\n",
    "                    #     \"metric_prompt_template\": \"\", # adding custom template\n",
    "                    # }\n",
    "                },\n",
    "                # Answer similarity metric is supported only when LLM as judge is configured. Uncomment only when using LLM as judge.\n",
    "                \"answer_similarity\": {\n",
    "                    # \"metric_prompt_template\": \"\", # adding custom template\n",
    "                    # Uncomment generative_ai_evaluator to use a different evaluator for this metric\n",
    "                    # Takes higher precedence than the generative_ai_evaluator specified at global level.\n",
    "                    # \"generative_ai_evaluator\": {  # metric specific LLM as judge configuration\n",
    "                    #     \"enabled\": True,\n",
    "                    #     \"evaluator_id\": evaluator_id,\n",
    "                    # },\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = wos_client.monitor_instances.mrm.execute_prompt_setup(\n",
    "    prompt_template_asset_id=SPACE_PROMPT_TEMPLATE_ID, \n",
    "    space_id=SPACE_ID,\n",
    "    label_column=label_column,\n",
    "    context_fields = context_fields,     \n",
    "    question_field = question_field,     \n",
    "    operational_space_id=operational_space_id, \n",
    "    problem_type=problem_type,\n",
    "    input_data_type=input_data_type, \n",
    "    supporting_monitors=monitors, \n",
    "    background_mode=False\n",
    ")\n",
    "\n",
    "result = response.result\n",
    "result.to_dict()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
